{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "qKifkaWtuWyY",
        "outputId": "45151cbf-ad0c-454d-bc82-c61cca122cc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0251946bea89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mVAE_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNILM_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'VAE_functions'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import argparse\n",
        "from VAE_functions import *\n",
        "from NILM_functions import *\n",
        "import pickle\n",
        "from scipy.stats import norm\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from dtw import *\n",
        "import logging\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ADD_VAL_SET = False\n",
        "\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "###############################################################################\n",
        "# Config\n",
        "###############################################################################\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--gpu\", default=0, type=int, help=\"Appliance to learn\")\n",
        "parser.add_argument(\"--config\", default=\"\", type=str, help=\"Path to the config file\")\n",
        "a = parser.parse_args()\n",
        "\n",
        "# Select GPU\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(a.gpu)\n",
        "\n",
        "print(\"###############################################################################\")\n",
        "print(\"NILM DISAGREGATOR\")\n",
        "print(\"GPU : {}\".format(a.gpu))\n",
        "print(\"CONFIG : {}\".format(a.config))\n",
        "print(\"###############################################################################\")\n"
      ],
      "metadata": {
        "id": "gvoBjxw9uwBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(a.config) as data_file:\n",
        "    nilm = json.load(data_file)\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "name = \"NILM_Disag_{}\".format(nilm[\"appliance\"])\n",
        "time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "for r in range(1, nilm[\"run\"]+1):\n",
        "    ###############################################################################\n",
        "    # Load dataset\n",
        "    ###############################################################################\n",
        "    x_train, y_train = load_data(nilm[\"model\"], nilm[\"appliance\"], nilm[\"dataset\"], nilm[\"preprocessing\"][\"width\"], nilm[\"preprocessing\"][\"strides\"], set_type=\"train\")\n",
        "    \n",
        "    main_mean = nilm[\"preprocessing\"][\"main_mean\"]\n",
        "    main_std = nilm[\"preprocessing\"][\"main_std\"]\n",
        "\n",
        "    app_mean = nilm[\"preprocessing\"][\"app_mean\"]\n",
        "    app_std = nilm[\"preprocessing\"][\"app_std\"]\n",
        "\n",
        "    ###############################################################################\n",
        "    # Training parameters\n",
        "    ###############################################################################\n",
        "    epochs = nilm[\"training\"][\"epoch\"]\n",
        "    batch_size = nilm[\"training\"][\"batch_size\"]\n",
        "\n",
        "    STEPS_PER_EPOCH = x_train.shape[0]//batch_size\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "                    float(nilm[\"training\"][\"lr\"]), \n",
        "                    decay_steps=STEPS_PER_EPOCH*nilm[\"training\"][\"decay_steps\"],\n",
        "                    decay_rate=1,\n",
        "                    staircase=False)\n",
        "\n",
        "    ###############################################################################\n",
        "    # Optimizer\n",
        "    ###############################################################################\n",
        "    def get_optimizer(opt):\n",
        "        if opt == \"adam\":\n",
        "            return tf.keras.optimizers.Adam(lr_schedule)\n",
        "        else:\n",
        "            return tf.keras.optimizers.RMSprop(lr_schedule)\n",
        "\n",
        "\n",
        "    ###############################################################################\n",
        "    # Create and initialize the model\n",
        "    ###############################################################################\n",
        "    if nilm[\"model\"] == \"VAE\":\n",
        "        model = create_model(nilm[\"model\"], nilm[\"config\"], nilm[\"preprocessing\"][\"width\"], optimizer=get_optimizer(nilm[\"training\"][\"optimizer\"]))\n",
        "    elif nilm[\"model\"] == \"DAE\":\n",
        "        model = create_model(nilm[\"model\"], nilm[\"config\"], nilm[\"preprocessing\"][\"width\"], optimizer=\"Adam\")\n",
        "    elif nilm[\"model\"] == \"S2P\":\n",
        "        model = create_model(nilm[\"model\"], nilm[\"config\"], nilm[\"preprocessing\"][\"width\"], optimizer=tf.keras.optimizers.Adam(learning_rate=nilm[\"training\"][\"lr\"], beta_1=0.9, beta_2=0.999))\n",
        "    elif nilm[\"model\"] == \"S2S\":\n",
        "        model = create_model(nilm[\"model\"], nilm[\"config\"], nilm[\"preprocessing\"][\"width\"], optimizer=tf.keras.optimizers.Adam(learning_rate=nilm[\"training\"][\"lr\"], beta_1=0.9, beta_2=0.999))\n",
        "\n",
        "    ###############################################################################\n",
        "    # Callback checkpoint settings\n",
        "    ###############################################################################\n",
        "    \n",
        "    list_callbacks = []\n",
        "    \n",
        "    # Create a callback that saves the model's weights\n",
        "    if nilm[\"training\"][\"save_best\"] == 1:\n",
        "        checkpoint_path = \"{}/{}/{}/logs/model/House_{}/{}/{}\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], nilm[\"dataset\"][\"test\"][\"house\"][0], time, r) +\"/checkpoint.ckpt\"\n",
        "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "        \n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         verbose=0,\n",
        "                                                         monitor=\"val_mean_absolute_error\",\n",
        "                                                         mode=\"min\",\n",
        "                                                         save_best_only=True)\n",
        "    else:\n",
        "        checkpoint_path = \"{}/{}/{}/logs/model/House_{}/{}/{}\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], nilm[\"dataset\"][\"test\"][\"house\"][0], time, r) +\"/cp-{epoch:04d}.ckpt\"\n",
        "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "        \n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         verbose=0,\n",
        "                                                         period=1)\n",
        "        \n",
        "    list_callbacks.append(cp_callback)\n",
        "\n",
        "    if nilm[\"training\"][\"patience\"] > 0:\n",
        "        patience = nilm[\"training\"][\"patience\"]\n",
        "        start_epoch = nilm[\"training\"][\"start_stopping\"]\n",
        "        \n",
        "        print(\"Patience : {}, Start at : {}\".format(patience, start_epoch))\n",
        "        \n",
        "        es_callback = CustomStopper(monitor='val_loss', patience=patience, start_epoch=start_epoch, mode=\"auto\")\n",
        "        \n",
        "        list_callbacks.append(es_callback)\n",
        "    \n",
        "    ###############################################################################\n",
        "    # Normalize Test Data and History Callback\n",
        "    ###############################################################################\n",
        "    if ADD_VAL_SET:\n",
        "        if nilm[\"dataset\"][\"name\"] == \"ukdale\":\n",
        "            if nilm[\"model\"] == \"S2P\":\n",
        "                x_test_s2p, y_test_s2p = transform_s2p(x_test, y_test, nilm[\"preprocessing\"][\"width\"], nilm[\"training\"][\"S2P_strides\"])\n",
        "                history_cb = AdditionalValidationSets([((x_test_s2p-main_mean)/main_std, (y_test_s2p-app_mean)/app_std, 'House_2')], verbose=1)\n",
        "            else:\n",
        "                history_cb = AdditionalValidationSets([((x_test-main_mean)/main_std, (y_test-app_mean)/app_std, 'House_2')], verbose=1)\n",
        "\n",
        "        elif nilm[\"dataset\"][\"name\"] == \"house_2\":\n",
        "            history_cb = AdditionalValidationSets([(x_test, y_test, 'House_2')], verbose=1)\n",
        "        elif nilm[\"dataset\"][\"name\"] == \"refit\":\n",
        "            history_cb = AdditionalValidationSets([(x_test, y_test, 'House_2')], verbose=1)\n",
        "\n",
        "        list_callbacks.append(history_cb)\n",
        "    \n",
        "    ###############################################################################\n",
        "    # Summary of all parameters\n",
        "    ###############################################################################\n",
        "    print(\"###############################################################################\")\n",
        "    print(\"Summary\")\n",
        "    print(\"###############################################################################\")\n",
        "    print(\"{}\".format(nilm))\n",
        "    print(\"Run number : {}/{}\".format(r,nilm[\"run\"]))\n",
        "    print(\"###############################################################################\")\n",
        "\n",
        "    if not os.path.exists(\"{}/{}/{}/logs/model/House_{}/{}\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], nilm[\"dataset\"][\"test\"][\"house\"][0], time)):\n",
        "        os.makedirs(\"{}/{}/{}/logs/model/House_{}/{}\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], nilm[\"dataset\"][\"test\"][\"house\"][0], time))\n",
        "\n",
        "    with open(\"{}/{}/{}/logs/model/House_{}/{}/config.txt\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], nilm[\"dataset\"][\"test\"][\"house\"][0], time), \"w\") as outfile:\n",
        "        json.dump(nilm, outfile)\n",
        "\n",
        "    ###############################################################################\n",
        "    # Train Model\n",
        "    ###############################################################################\n",
        "    if nilm[\"dataset\"][\"name\"] == \"ukdale\":\n",
        "        ###############################################################################\n",
        "        # Real Validation\n",
        "        ###############################################################################\n",
        "        if nilm[\"model\"] == \"S2P\":\n",
        "            x_train_s2p, y_train_s2p = transform_s2p(x_train, y_train, nilm[\"preprocessing\"][\"width\"], nilm[\"training\"][\"S2P_strides\"])\n",
        "            \n",
        "            history = model.fit((x_train_s2p-main_mean)/main_std, (y_train_s2p-app_mean)/app_std, validation_split=nilm[\"training\"][\"validation_split\"], shuffle=True, \n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=list_callbacks, verbose=1, initial_epoch=0)\n",
        "\n",
        "        elif nilm[\"model\"] == \"VAE\":    \n",
        "            history = model.fit((x_train-main_mean)/main_std, (y_train-app_mean)/app_std, validation_split=nilm[\"training\"][\"validation_split\"], shuffle=True, \n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=list_callbacks, verbose=1, initial_epoch=0)\n",
        "            \n",
        "        elif nilm[\"model\"] == \"S2S\":    \n",
        "            history = model.fit((x_train-main_mean)/main_std, (y_train-app_mean)/app_std, validation_split=nilm[\"training\"][\"validation_split\"], shuffle=True, \n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=list_callbacks, verbose=1, initial_epoch=0)\n",
        "            \n",
        "        elif nilm[\"model\"] == \"DAE\":    \n",
        "            history = model.fit((x_train-main_mean)/main_std, (y_train-app_mean)/app_std, validation_split=nilm[\"training\"][\"validation_split\"], shuffle=True, \n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=list_callbacks, verbose=1, initial_epoch=0)\n",
        "\n",
        "        ###############################################################################\n",
        "        # Save history\n",
        "        ###############################################################################\n",
        "        np.save(\"{}/{}/{}/logs/model/House_{}/{}/{}/history.npy\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], nilm[\"dataset\"][\"test\"][\"house\"][0], time, r), history.history)\n",
        "        #np.save(\"{}/{}/{}/logs/model/{}/{}/history_cb_{}.npy\".format(name, nilm[\"dataset\"][\"name\"], nilm[\"model\"], time, r, epochs), history_cb.history)\n",
        "\n",
        "        print(\"Fit finished!\")\n",
        "    else:\n",
        "        print(\"Error in dataset name!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "V4U23A80uznu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}